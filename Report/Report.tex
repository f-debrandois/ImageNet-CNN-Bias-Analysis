\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} % Use UTF-8 encoding for input
\usepackage{babel} 
	       
\usepackage{lmodern}	
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{stmaryrd}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[upgreek]{txgreeks}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[text={15cm,24.5cm},centering]{geometry}


% Définir la fonction pour créer une boîte de code
\newcommand{\code}[1]{%
    \begin{tcolorbox}[colback=black!10!white,colframe=black]
        #1
    \end{tcolorbox}
}


\begin{document}

\begin{figure}[t]
    \centering
    \includegraphics[width=5cm]{src/inp_n7.png}
    \hfill
    \includegraphics[width=3.8cm]{src/insa_toulouse.png}
\end{figure}

\title{\vspace{4cm} \textbf{Kaggle project report}}

\author{by Felix Foucher de Brandois}
        
\date{\vfill Formation ModIA - INSA, 4$^e$ année \\
2023-2024}

\maketitle

\newpage
\tableofcontents
\listoffigures

\newpage


\section{Introduction}
Deep learning models are widely used for image classification.
The goal of this project is to compare state-of-the-art models on a subset of ImageNet dataset.
Then, we will evaluate the impact of bias in the dataset on the models' predictions.
The link of the project is : \url{https://www.kaggle.com/competitions/modia-ml-2024/overview}.


\section{Train state-of-the-art CNN models}

The goal is to achieve a good classification accuracy on the test dataset, using Pytorch.

\subsection{Data preprocessing and augmentation}

Using the given dataset (4000 images), I started by defining basic data transformations that will be applied to the images :

\begin{itemize}
    \item \texttt{Grayscale} : convert the image to grayscale
    \item \texttt{CenterCrop} : crop the image to the center so the images have the same size
    \item \texttt{RandomHorizontalFlip} : randomly flip the image horizontally
    \item \texttt{RandomVerticalFlip} : randomly flip the image vertically
    \item \texttt{RandomRotation} : randomly rotate the image
\end{itemize}



\subsection{Model architecture}

I used two different models in this project to compare their performances : LeNet and ResNet.

\subsubsection{LeNet}

The LeNet architecture is as follows :

\begin{itemize}[label=-]
    \item Input layer : 1 channel, 256x256 pixels
    \item Convolutional layer 1 : 6 filters, kernel size 5x5, stride 1, padding 0
    \item Max pooling layer 1 : kernel size 2x2, stride 2
    \item Convolutional layer 2 : 16 filters, kernel size 5x5, stride 1, padding 0
    \item Max pooling layer 2 : kernel size 2x2, stride 2
    \item Fully connected layer 1 : 120 neurons
    \item Fully connected layer 2 : 84 neurons
    \item Output layer : 4 neurons (number of classes)
\end{itemize}

The activation function used is the ReLU function.

\code{
    class LeNet(nn.Module):\\
    \indent\qquad def \_\_init\_\_(self, num\_channels=1, num\_classes=4): \\
    \indent\qquad \qquad super(LeNet, self).\_\_init\_\_() \\
    \indent\qquad \qquad self.conv1 = nn.Conv2d(num\_channels, 6, kernel\_size=5, padding=2) \\
    \indent\qquad \qquad self.conv2 = nn.Conv2d(6, 16, kernel\_size=5) \\
    \indent\qquad \qquad self.fc1   = nn.Linear(62*62*16, 120) \\
    \indent\qquad \qquad self.fc2   = nn.Linear(120, 84) \\
    \indent\qquad \qquad self.fc3   = nn.Linear(84, num\_classes) \\

    \indent\qquad def forward(self, x): \\
    \indent\qquad \qquad x = F.max\_pool2d(F.relu(self.conv1(x)), kernel\_size=(2, 2), stride=2) \\
    \indent\qquad \qquad x = F.max\_pool2d(F.relu(self.conv2(x)), kernel\_size=(2, 2), stride=2) \\
    \indent\qquad \qquad x = nn.Flatten()(x) \\
    \indent\qquad \qquad x = F.relu(self.fc1(x)) \\
    \indent\qquad \qquad x = F.relu(self.fc2(x)) \\
    \indent\qquad \qquad x = self.fc3(x) \\
    \indent\qquad \qquad return x
}


\subsubsection{ResNet}

The ResNet architecture is as follows :

\begin{itemize}[label=-]
    \item Input layer : 1 channel, 256x256 pixels
    \item Convolutional layer 1 : 64 filters, kernel size 7x7, stride 2, padding 3
    \item Max pooling layer 1 : kernel size 3x3, stride 2
    \item Residual block 1 : 3 blocks, 64 filters
    \item Residual block 2 : 4 blocks, 128 filters
    \item Residual block 3 : 6 blocks, 256 filters
    \item Residual block 4 : 3 blocks, 512 filters
    \item Average pooling layer : kernel size 8x8, stride 1
    \item Fully connected layer : 4 neurons (number of classes)
\end{itemize}

To implement the ResNet model, I used the Pytorch library :

\code{
    class ResNet(nn.Module):\\
    \indent\qquad def \_\_init\_\_(self, num\_channels=1, num\_classes=4): \\
    \indent\qquad \qquad super(ResNet, self).\_\_init\_\_() \\
    \indent\qquad \qquad self.resnet = torchvision.models.resnet18(pretrained=False) \\
    \indent\qquad \qquad self.resnet.conv1 = nn.Conv2d(num\_channels, 64, kernel\_size=7, stride=2, padding=3, bias=False) \\
    \indent\qquad \qquad self.resnet.fc = nn.Linear(512, num\_classes) \\
    
    \indent\qquad def forward(self, x): \\
    \indent\qquad \qquad return self.resnet(x)
}

\subsection{Training}

I trained the models using the Stochastic Gradient Descent (SGD) optimizer. \\
I splitted the dataset into a training set (80\%) and a validation set (20\%).


\subsection{Results}

\subsubsection{LeNet}

I didn't explored the hyperparameters of the LeNet model, since the best accuracy I got on the validation set around 50\%.
However, here are the results :\\

\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Train acc} & \textbf{Val acc} & \textbf{Nb of epochs} & \textbf{Batch size} & \textbf{Learning rate} & \textbf{Training time} \\
    \hline
    53.91\% & 46.29\% & 10 & 32 & 0.01 & 1m46 \\
    \hline
    42.87\% & 39.79\% & 10 & 32 & 0.001 & 1m15 \\
    \hline
    53.77\% & 42.05\% & 50 & 32 & 0.01 & 5m23 \\
    \hline
\end{tabular}\\

The following figure shows the evolution of the training loss and validation accuracy over the epochs :

\begin{figure}[ht]
    \centering
    \includegraphics[width=10cm]{src/LeNet1.png}
    \caption{LeNet train and validation loss}
\end{figure}

We can see that the model is too simple to capture the underlying patterns in the data.
The training loss and validation loss are decreasing over the epochs, but the validation accuracy is not increasing.
We will therefore use a more complex model, such as ResNet, to improve the accuracy.

\subsubsection{ResNet}

I explored the hyperparameters of the ResNet model, and I got the best accuracy on the validation set around 80\%.
Here are the results :\\

\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Train acc} & \textbf{Val acc} & \textbf{Nb of epochs} & \textbf{Batch size} & \textbf{Learning rate} & \textbf{Training time} \\
    \hline
    63.64\% & 54.86\% & 10 & 32 & 0.01 & 1m31 \\
    \hline
    51.97\% & 50.89\% & 10 & 32 & 0.001 & 1m11 \\
    \hline
    63.34\% & 62.40\% & 10 & 64 & 0.01 & 53s \\
    \hline
    72.06\% & 69.13\% & 20 & 32 & 0.01 & 8m12 \\
    \hline
\end{tabular}\\

The following figure shows the evolution of the training loss and validation accuracy over the epochs for the best model (80\% accuracy) :

\begin{figure}[ht]
    \centering
    \includegraphics[width=10cm]{src/Resnet1.png}
    \caption{ResNet train and validation loss}
\end{figure}

We can see that the model is not overfitting, since the training loss and validation loss are decreasing over the epochs.


\section{Machine learning and AI safety}

\subsection{Bias introduction in ImageNet}

We try to understand how an image classification model (such as the ones trained in section 2) can be biased towards a specific outcome.
To make things easier we consider the binary classification problem (2 classes). \\
We propose to implement the following setup :


\subsubsection{Biased Dataset Construction}

We decide to concatenate to each of our images $x$ from ImageNet a new set of variables $\epsilon$.
As we shall specify, the second variable $\epsilon$ is a noise-like image, which is not directly computed from $x$.
Yet, it introduces some bias as we choose the value of $\epsilon$ to be strongly correlated to the label ($0$ or $1$) of the images.\\

More precisely, we assume $(\tilde{x}, y) = ([x, \epsilon], y)$ is a random sample of the modified dataset.  
We shall specify the value of $\epsilon$ using $y$. Let $p_0 \in [0, 1], p_1 \in [0, 1]$ be two probabilities.  
Given $(\tilde{x}, y)$, the bias variable $S$ is defined as
$$
S \sim Bernoulli(p_k), \quad \text{ if } y = k, k \in \{0, 1\}.
$$

Then, we choose $\epsilon$ according to $S$ as below :
\begin{itemize}[label=--]
    \item $\epsilon = 0$ if $S = 0$.
    \item $\epsilon \sim \mathcal{N}(0, \sigma^2I)$ if $S = 1$.
\end{itemize}

In the extreme case where $p_0 = 0$ and $p_1 = 1$, one can ignore the original image $x$ and only use $\epsilon$ to predict $y$.
In that case, our predictions are never using the image $x$ and we are only relying on the noise-like $\epsilon$ that we introduced to the dataset.


\subsubsection{Model Bias Evaluation}

We compare two different settings :
\begin{itemize}[label=--]
    \item $\textit{model}_1$ is trained on the unaltered ImageNet dataset
    \item $\textit{model}_2$ is trained on the biased version of ImageNet (where we append the biased variables $\epsilon$ to the images in a separate channel)
\end{itemize}


We finally compare the actual bias in both $\textit{model}_1$ and $\textit{model}_2$.\\

Assume $\hat{y}$ is the prediction of a model based on $\tilde{x}$.
We shall separate the dataset set into $2$ groups, one with $S = 0$, the other with $S = 1$.
The bias of this model can be computed from a ratio of these two groups, using the $\underline{\text{DI metric}}$ defined as :
$$
DI = \frac{P(\hat{y} = 1 | S = 0)}{P(\hat{y} = 1 | S = 1)}.
$$

In practice, a model is considered unbiased as long as the DI metric is close to $1$.


\begin{itemize}
    \item I compared the bias of the two models using the DI metric.
    \begin{itemize}[label=-]
        \item For the unbiased model, I created a dataset with $p_0 = 0.5$ and $p_1 = 0.5$. \\
        I got a DI metric of 0.87 which is close to 1, so the model is unbiased.

        \item For the biased model, I created a dataset with $p_0 = 0.1$ and $p_1 = 0.9$. \\
        I got a DI metric of 0.23 which is far from 1, so the model is biased.\\
    \end{itemize}

    \item I also compared the accuracy scores between the two models.\\
    The accuracy of the biased model is higher than the accuracy of the unbiased model.
    This is due to the fact that the biased model is using the noise-like $\epsilon$ to predict the label, which is strongly correlated to the label.
\end{itemize}


\section{Conclusion}

In this project, I trained two different models (LeNet and ResNet) on a subset of ImageNet dataset.
I compared their performances and evaluated the impact of bias in the dataset on the models' predictions.
The results show that the ResNet model has a better accuracy than the LeNet model.\\
I also worked on the impact of bias in the dataset on the models' predictions, using the DI metric.




\end{document}